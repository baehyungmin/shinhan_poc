import streamlit as st
import asyncio
import os
# from dotenv import load_dotenv

# from mcp.client.stdio import stdio_client # â¬…ï¸ mcp ê´€ë ¨ ë¡œì§ ì œê±°
# from mcp import ClientSession, StdioServerParameters # â¬…ï¸ mcp ê´€ë ¨ ë¡œì§ ì œê±°
# from langchain_mcp_adapters.tools import load_mcp_tools # â¬…ï¸ mcp ê´€ë ¨ ë¡œì§ ì œê±°
# from langgraph.prebuilt import create_react_agent # â¬…ï¸ mcp/langgraph ê´€ë ¨ ë¡œì§ ì œê±°

from google import genai
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# â¬…ï¸ embedding model ì¶”ê°€
from langchain_openai import OpenAIEmbeddings # â¬…ï¸ Langchainì˜ ìµœì‹  OpenAI ì„ë² ë”© ëª¨ë“ˆ
from pymilvus import MilvusClient # â¬…ï¸ Milvus client ì¶”ê°€

from PIL import Image
from pathlib import Path

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
ZILLIZ_URI = st.secrets["ZILLIZ_URI"]
ZILLIZ_TOKEN = st.secrets["ZILLIZ_TOKEN"]

EMBEDDING_MODEL='text-embedding-3-small'
EMBEDDING_DIMENSION =1536
COLLECTION_NAME='shinahn_collection'
MODEL_NAME = 'gemini-2.5-flash'

milvus_client = MilvusClient(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)
gemini_client  = genai.Client()

# í™˜ê²½ë³€ìˆ˜
ASSETS = Path("assets")
# GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
# # â¬…ï¸ OpenAI ë° Milvus ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€ (st.secretsì— ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
# OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"] # OpenAI API Key
# MILVUS_URI = st.secrets["MILVUS_URI"]
# MILVUS_TOKEN = st.secrets["MILVUS_TOKEN"]
# COLLECTION_NAME = st.secrets["COLLECTION_NAME"]

# # ëª¨ë¸ ë° ì‹œìŠ¤í…œ ì„¤ì •
# MODEL_NAME = 'gemini-2.5-flash'
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•ê³¼ ì±„ë„, ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ, ë¶„ì„ ê²°ê³¼ì—ëŠ” ê°€ëŠ¥í•œ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
GREETING = "ë§ˆì¼€íŒ…ì´ í•„ìš”í•œ ê°€ë§¹ì ì„ ì•Œë ¤ì£¼ì„¸ìš” \n(ì¡°íšŒê°€ëŠ¥ ì˜ˆì‹œ: ë™ëŒ€*, ìœ ìœ *, ë˜¥íŒŒ*, ë³¸ì£½*, ë³¸*, ì›ì¡°*, í¬ë§*, í˜ì´*, Hì»¤*, ì¼€í‚¤*)"

# ---------------------------------------------------------------------------------
# ğŸ” Embedding ë° Retrieval í•¨ìˆ˜
# ---------------------------------------------------------------------------------

embedding_model = OpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        model=EMBEDDING_MODEL,
        # 'dimensions' íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê¸¸ì´ë¥¼ 1536ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
        dimensions=EMBEDDING_DIMENSION
)

@st.cache_resource
def embed_query(query: str, embedding_model):
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    # Langchain Embeddings ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©: embed_query
    return embedding_model.embed_query(query)

def retrieve_from_milvus(query_vector: list):
    """Milvus Cloudì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ top_k=1ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        search_vectors = [query_vector]
        output_fields = ["text", "description"]

        # Milvus ê²€ìƒ‰
        search_result = milvus_client.search(
            collection_name=COLLECTION_NAME,
            data=search_vectors,      # ê²€ìƒ‰í•  ì¿¼ë¦¬ ë²¡í„°
            limit=1,                   # ìƒìœ„ 1ê°œì˜ ê²°ê³¼ë§Œ ê°€ì ¸ì˜´ (ìš”ì²­ì— ë”°ë¼ top_k=1)
            output_fields=output_fields # â¬…ï¸ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨í•  í•„ë“œ ì§€ì •
        )

        # ê²°ê³¼ íŒŒì‹±: ìš”ì²­ëœ í˜•ì‹ì— ë”°ë¼ ê°€ì¥ ì²« ë²ˆì§¸ ì—”í‹°í‹°ì˜ 'description' í•„ë“œ ë°˜í™˜
        if search_result and search_result[0]:
            # result = search_result[0][0]['entity']['description']
            # description í•„ë“œê°€ ê²€ìƒ‰ ê²°ê³¼ì˜ RAG ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¼ê³  ê°€ì •
            result = search_result[0][0]['entity']['description']
            return result
        return None

    except Exception as e:
        st.error(f"Milvus ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


@st.cache_data
def load_image(name: str):
    return Image.open(ASSETS / name)

st.set_page_config(page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬")

def clear_chat_history():
    # SYSTEM_PROMPT ìƒìˆ˜ë¡œ ë³€ê²½
    st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT), AIMessage(content=GREETING)]

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.image(load_image("shc_ci_basic_00.png"), width='stretch')
    st.markdown("<p style='text-align: center;'>2025 Big Contest</p>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center;'>AI DATA í™œìš©ë¶„ì•¼</p>", unsafe_allow_html=True)
    st.write("")
    col1, col2, col3 = st.columns([1,2,1])
    with col2:
        st.button('Clear Chat History', on_click=clear_chat_history)

# í—¤ë”
st.title("ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ğŸ”‘ ë¹„ë°€ìƒë‹´ì†Œ")
st.subheader("#ìš°ë¦¬ë™ë„¤ #ìˆ¨ì€ë§›ì§‘ #ì†Œìƒê³µì¸ #ë§ˆì¼€íŒ… #ì „ëµ .. ğŸ¤¤")
st.image(load_image("image_gen3.png"), width='stretch', caption="ğŸŒ€ ë¨¸ë¦¬ì•„í”ˆ ë§ˆì¼€íŒ… ğŸ“Š ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ?")
st.write("")

# ë©”ì‹œì§€ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        AIMessage(content=GREETING)
    ]

# ì´ˆê¸° ë©”ì‹œì§€ í™”ë©´ í‘œì‹œ
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.write(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.write(message.content)

def render_chat_message(role: str, content: str):
    with st.chat_message(role):
        st.markdown(content.replace("<br>", " \n"))



# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ 
async def generate_answer_with_description_rag(gemini_client, user_query: list):
    """
    ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ ë‚´ìš© + description ë©”íƒ€ë°ì´í„°)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Geminiì—ê²Œ ë‹µë³€ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    query_vector = embed_query(query, embedding_model)
    full_context = retrieve_from_milvus(query_vector)


    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
      ì•„ë˜ì˜ ë°ì´í„°ëŠ” JSON í˜•ì‹ì˜ ë°ì´í„°ë¡œ íŠ¹ì • ê°€ë§¹ì ì˜ ì •ë³´ì™€ ê·¸ ê°€ë§¹ì ì˜ ìµœê·¼ 24 ê°œì›”ê°„ì˜ ì›”ë³„ ì´ìš© ì •ë³´ì™€ ì›”ë³„ ì´ìš© ê³ ê° ì •ë³´ ë°ì´í„°ì´ë©° ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ì´ ë˜ì–´ ìˆì–´.
      - ê°’ì´ -999999.99 ì¸ ê²½ìš° ì •ë³´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì„
      - 'ê°€ë§¹ì  ìš´ì˜ ê°œì›”ìˆ˜ êµ¬ê°„': 0% ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì„(ìš´ì˜ê°œì›” ìˆ˜ê°€ ìƒìœ„ ì„)
      - 'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„': 0% ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì„(ë§¤ì¶œ ê¸ˆì•¡ì´ ìƒìœ„ ì„)
      - 'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„': 0% ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì„(ë§¤ì¶œ ê±´ìˆ˜ê°€ ìƒìœ„ ì„)
      - 'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„': 0% ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì„(unique ê³ ê° ìˆ˜ê°€ ìƒìœ„ì„)
      - 'ê°ë‹¨ê°€ êµ¬ê°„': 0% ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì„(ê°ë‹¨ê°€ê°€ ìƒìœ„ ì„)
      - 'ì·¨ì†Œìœ¨ êµ¬ê°„': 1 êµ¬ê°„ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì·¨ì†Œìœ¨ì´ ë‚®ìŒ
      - 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨': ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê¸ˆì•¡ í‰ê·  ëŒ€ë¹„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë§¤ì¶œ ê¸ˆì•¡ ë¹„ìœ¨ì´ë©° í‰ê· ê³¼ ë™ì¼í•  ê²½ìš° 100 ì´ì•¼.
      - 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨': ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê±´ìˆ˜ í‰ê·  ëŒ€ë¹„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë§¤ì¶œ ê±´ìˆ˜ ë¹„ìœ¨ì´ë©° í‰ê· ê³¼ ë™ì¼í•  ê²½ìš° 100 ì´ì•¼.
      - 'ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨': ('ì—…ì¢… ë‚´ ìˆœìœ„'/'ì—…ì¢… ë‚´ ì „ì²´ ê°€ë§¹ì '* 100) ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ 0 ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì— ë­í‚¹ë˜ëŠ”ê±°ì•¼.
      - 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨': ('ìƒê¶Œ ë‚´ ìˆœìœ„'/'ìƒê¶Œ ë‚´ ì „ì²´ ê°€ë§¹ì '* 100) ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ 0 ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„ì— ë­í‚¹ë˜ëŠ”ê±°ì•¼.

      ì•„ë˜ì˜ data ë¥¼ ë¶„ì„í•´ì„œ í•´ë‹¹ ê°€ë§¹ì ì˜ ë§¤ì¶œ ì „ëµì„ ì œì•ˆí•´ì¤˜.

    {full_context}

    ---

    QUERY:
    {user_query}
    """

    response = gemini_client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config={
            "temperature": 0.1 # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ë¥¼ ì‚¬ìš©
        }
    )

    return response.text


# ì‚¬ìš©ì ì…ë ¥ ì°½
if query := st.chat_input("ê°€ë§¹ì  ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append(HumanMessage(content=query))
    render_chat_message("user", query)

    with st.spinner("Thinking..."):
        try:
            # RAG ë¡œì§ ì‹¤í–‰
            # process_user_input_rag í•¨ìˆ˜ëŠ” ë‚´ë¶€ì—ì„œ ë™ê¸° í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ, asyncio.runì„ ì œê±°í•˜ê³  ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ Streamlitì˜ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ process_user_input_ragë¥¼ asyncë¡œ ìœ ì§€í•˜ê³  asyncio.runì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            # RAG í•¨ìˆ˜ëŠ” ì‹¤ì œ ë¹„ë™ê¸° I/Oë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì‚¬ì‹¤ìƒ `await process_user_input_rag(query)`ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ ë” ìì—°ìŠ¤ëŸ½ì§€ë§Œ
            # ê¸°ì¡´ ì½”ë“œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ asyncio.runì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

            # ê¸°ì¡´ ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©°, RAG í•¨ìˆ˜ëŠ” asyncë¡œ ì •ì˜í•©ë‹ˆë‹¤.
            # st.chat_input ì½œë°±ì€ ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì´ë¯€ë¡œ asyncio.runì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            reply = asyncio.run(generate_answer_with_description_rag(gemini_client=gemini_client,user_query=query))

            st.session_state.messages.append(AIMessage(content=reply))
            render_chat_message("assistant", reply)

        except Exception as e:
            # ë‹¨ì¼ ì˜ˆì™¸ ì²˜ë¦¬
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e!r}"
            st.session_state.messages.append(AIMessage(content=error_msg))
            render_chat_message("assistant", error_msg)